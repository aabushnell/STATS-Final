---
title: "technical report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


I. ABSTRACT


We created a web application, PicBook, that displays image representations of the most commonly tweeted nouns in any given location. By using API data from Twitter, Google Maps, and a word-to-icon generating site called The Noun Project, PicBook generates and displays pictorial icons depicting the most tweeted nouns in any user-entered region (address or coordinates/radius). The app, which is precoded with a list of 100 common nouns, sends the user-input location through the Google Places API, the Twitter API, and the Noun Project API to diplay the region's top 3 most commonly tweeted nouns (of the 100) and their corresponding icons. To exemplify the utility of our app, we built additional tabs into the Shiny interface, which display static graphics that show different temporal and spatial metrics for a set of precollected data.


II. INTRODUCTION


Pictorial writing is one of the oldest forms of written communication that dates farther back in history than spoken language. Conversely, social media is one of the most widely used forms of communication in the modern age. What can reconciling these two form of written language tell us about the relationships between words and pictures? How can we use pictorial representations to better understand the impact words and their usages? 

To examine this, we built a Shiny app, Picbook, that converts words used in Twitter posts into pictorial icons. Our app, precoded with a list of 100 common nouns, allows the user to type in a location, either an address or coordinates/radius; when entered, it outputs the top 3 of the 100 nouns most frequently used in the (approximately) 500 most recent tweets sent from that location. The application inputs the user-entered region through the Google Places API, which geographically identifies and generates a geocode for that region. The geocode is then input through the Twitter API, which identifies the most recent tweets (a specified number) published within that geographical region. Nouns are then parsed out from those tweets; of the 100 pre-coded nouns, the three most frequently used are looked up in the The Noun Projectâ€™s API data, which finally generates and displays the icons corresponding to those nouns. The app also includes 3 tabs that display different temporal and spatial metrics: the proportion of usage of a selected word across states, the usage frequency of a selected word across different times in a day, and the top most used word in major cities worldwide. 

Through this app and the visuals we produced through it, we aimed to accomplish 3 goals: to create a dynamic tool that can answer a variance of questions about most tweeted words across regions; to create an evocative type of output that presents data for a popular social tool in an engaging way; and to examine the way in which visuals can increase or change the impact of results. 


III. DATA


First, the following packages must be installed and libraries loaded.
```{r}
install.packages("httr")
install.packages("rtweet")
install.packages('maps')
install.packages("magick")

library(httr)
library(rtweet)
library(readr)
library(mdsr)
library(tidyr)
library(maps)
library(magick)
```

In order to run this code properly, several API keys and tokens are required. Make sure you have access to the Google Places API (https://developers.google.com/places/web-service/intro), the standard Twitter APIs through a developer account (https://developer.twitter.com/en/docs/basics/getting-started), and the Noun Project api through a Noun Project account (https://api.thenounproject.com/getting_started.html). Then, make a file called `config.R` with the following variables saved with the appropriate values (as strings):
```{r}
 google_key <- '*Insert Google Places api key here*'
 twitter_key <- '*Insert Twitter api public key here*'
 twitter_secret_key <- '*Insert Twitter api secret key here*'
 twitter_token <- '*Insert Twitter api public token here*'
 twitter_secret_token <- '*Insert Twitter api secret token here*'
 nouns_key <- '*Insert nounproject api public key here*'
 nouns_secret_key <- '*Insert nounproject api secret key here*'
```

Then, one can load these values locally.
```{r}
source('config.R')
```

In order to generate geographic boundaries to search for tweets in we made use of the Google Places API. Using the `lookup_coords' function from `rtweet` one can search with some address criterion and get a boundary defined by coordinates.
```{r}
address <- '#Enter address here'
q <- lookup_coords(address, apikey = google_key)
```

One can also manually generate a search region defined by a single point (and its latitude and longitude) and a radius.
```{r}
lat <- '#Latitude number here'
long <- '#Longitude number here'
radius <- '#Radius number here'
q <- paste(lat, long, paste(radius, "km", sep = ""), sep = ",")
```

We gather our tweet data from the Twitter API with the `search_tweets` function from the `rtweet` package. First we define a token with all the appropriate key and token values (defined in `config.R`), then we run the `search_tweets` function for our geocode of `q` defined above. The result is a df with many columns but we are only concerned with the `text` column that contains the actual content of each tweet from our sample. The `search_tweets` function returns a number of recent tweets for the given geographic area. If the geographic are is defined to be small, the returned number of tweets may be smaller than the defined maximum number of tweets to be searched (`tweet_n`).
```{r}
create_token(app = "PicBook",
             consumer_key = twitter_key,
             consumer_secret = twitter_secret_key,
             access_token = twitter_token,
             access_secret = twitter_secret_token)

# Runs search of recent tweets
tweet_n <- 500 #Default, can change
rt <- search_tweets(
 "lang:en", geocode = q, n = tweet_n)
```

We've compiled a list of 100 common English nouns from https://www.espressoenglish.net/100-common-nouns-in-english/. The file consists of a single column, Words, with each word listed. Other files of nouns (or other words) saved in a similar formate could also be used instead, we simply chose a relatively short list in the interest of processing time.
```{r}
words_file <- read_csv('words.csv')
# Converts the words columns into a list for later reference
words <- words_file$Words
```

We also make use of the Noun Project API. This has been done using the `rtweet` package. First we define an oauth_app (a data type that is essentially a public and private key packaged together) and then define a function (taken from https://community.rstudio.com/t/getting-httr-to-work-with-the-noun-project-api/16165/2) to run GET requests through the API. We use GET requests with an endpoint to search for icons with a given name. This GET request returns a large amount of metadata but we only use a link to a url displaying the returned icon contained within this result.
```{r}
nouns_app <- oauth_app("nouns_api", nouns_key, nouns_secret_key)

get_nouns_api <- function(endpoint, baseurl = "http://api.thenounproject.com/", app) {
  url <- modify_url(baseurl, path = endpoint)
  info <- oauth_signature(url, app = app)
  header_oauth <- oauth_header(info)
  GET(url, header_oauth)
} 
```

Using the tweet data obtained from the Twitter API, we then used the words list loaded earlier to search for matches between words in the list and words in the tweets from our tweets df. This was done for each tweet in the df. Then, we added up the number of matches for each word over the entire sample of tweets. From there, we defined two functions: one that defines a list with the top n words by number of matches and one that defines a list with urls leading to .pngs with icons (from Noun Project API) that represent the top n matches. These urls are generated, for each top word, using the `get_nouns_api` function defined earlier, extracting the `preview_url` value from the result.
```{r}
match_words <- function(tweet){
  matched_values <- sapply(words, grepl, tweet)
  return(matched_values)
}

matched_words <- data.frame(mapply(match_words, rt$text))  %>%
  cbind(rownames(text_matches)) %>%
  rename(word = 'rownames(text_matches)') %>%
  mutate(occurences = rowSums(. == TRUE)) %>%
  select(word, occurences) %>%
  arrange(desc(occurences)) %>%
  select(word)

top_n_words <- function(words, n) {
  # Defines list with the n most common words
  top_words <- list()
  for (i in 1:n) {
    top_words[i] <- as.character(words$word)[i]
  }
  return(top_words)
}

top_n_urls <- function(words, n){
  # Generates endpoints for words from top_n_words function and then queries a response from nounproject API
  # A url with a link to a .png of the corresponding icon is then taken from this response
  urls <- list()
  for (i in 1:n) {
    endpoint <- paste("icon", as.character(words$word)[i], sep = "/")
    res <- get_nouns_api(endpoint = endpoint, app = nouns_app)
    icon_res <- content(res,"parsed")
    urls[i] <- icon_res$icon$preview_url
  }
  
  return(urls)
}
```

In addition to our dynamic, interactive generation of icons for the top tweeted words we also prepared static data on the usage of the same words in tweets. We used the same `search_tweets` function for this, running a function that searched for tweets from each US state, then merged all the results together into one big df. Then, we merged this data with map data (from ggplot and the `maps` package) in order to create a map plot by states. This `matched_state_data` is what was saved as `state_data.csv` and used in our app. The data was found at 3PM EST, 12/10/18.
```{r}
state_data <- read_csv('state_data.csv')
# Function that searches for a tweet sample for each US state, then organizes them together into one df 
state_data <- data.frame(words) %>%
  rename(word = words)
for (name in state.name) {
  q = lookup_coords(name, apikey = google_key)

  rt <- search_tweets(
  "lang:en", geocode = q, n = 250)
  
  num_rows <- rt %>%
    summarise(n())
  num_rows$`n()`
  
  rt_words <- data.frame(mapply(match_words, rt$text))  %>%
    cbind(rownames(text_matches)) %>%
    rename(word = 'rownames(text_matches)') %>%
    mutate(occurences = rowSums(. == TRUE)) %>%
    select(word, occurences) %>%
    arrange(desc(occurences)) %>%
    mutate(occurences = occurences / num_rows$`n()`) %>%
    inner_join(state_data, by = 'word') %>%
    rename(!!name := occurences)
  state_data <- rt_words
  
}

# Calls state geodata from maps package
states <- map_data("state")
# Adds our data to states geodata
matched_state_data <- states %>%
 rename(map_group = group) %>%
 full_join(state_data, by = c('region' = 'State'))
```

We also ran the same `search_tweets` function (for `address='california'`) every hour from 6AM-7PM PST, 12/10/18. Each hours result was saved under a different name corresponding to the hour it was searched (i.e `rt_8`) and then the following code was used to merge together the hour data, changing the numbers in the code to match the appropriate hour. The `time_data_trans` result was saved as `time_data.csv` to be used by the app.
```{r}
time_data_read <- read_csv('time_data.csv')

time_data <- data.frame(words) %>%
  rename(word = words)

num_rows <- rt_19 %>%
    summarise(n())
  num_rows$`n()`
  
rt_words <- most_common_words(rt_19) %>%
  mutate(occurences = occurences / num_rows$`n()`) %>%
  inner_join(time_data, by = 'word') %>%
  rename('19' = occurences)
time_data <- rt_words

time_data_trans <- time_data %>%
  gather(Time, proportion, -word) %>%
  spread(word, proportion) %>%
  mutate(Time = as.numeric(Time))
```


IV. RESULTS


Are results from our data are meant to be more artistic than scientific. They are meant to evoke an intellectual and emotional connection to the data from our sample. We want to show how a bunch of random tweets can be distilled down into simple pictures forming a framework for the mind to feel a connection to the results as it imagines deeper meaning. Our point is that presenting data in the way we do, using simple nouns and pictographic representation, allows for a deep humanistic connection to data, even if no high level technical analysis is done.

With the following code, one can search for the top nouns, and display a specific image from the listed, identified by ordinal number. In our app we chose to display the top three images/nouns for each region searched. 
```{r}
total_noun_num <- 3 #Change to search for more icons
url_list <- top_n_urls(matched_words, total_noun_num)

current_noun_num <- 1 #Change to see other icons
icon <- image_read(url_list[current_noun_num])
print(icon)
```

Moreover, for a selected word from our words list, we created the following plot showing the proportion of tweets with that word appearing in them per state. This was done using the `state_data` defined earlier. Using this interactive (at least in our shiny app) one can see how in any given moment, the content of tweets can vary significantly by state. The results displayed in map format, coupled with any American's preconcieved perception of the unique character of each state easily leads us to jump to conclusions about the results displayed here, with a story behind the data easy to imagine.
```{r}
selected_word_state <- '# Enter word here'

ggplot(state_data, aes_string(x = 'long', y = 'lat', fill = selected_word_state, group = 'map_group'), color = "white") + 
    coord_fixed(1.3) +
    geom_polygon(alpha = 0.7) +
    geom_polygon(color = "black", fill = NA) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5),
      axis.text = element_blank(),
      axis.line = element_blank(),
      axis.ticks = element_blank(),
      panel.border = element_blank(),
      panel.grid = element_blank(),
      axis.title = element_blank()) +
    labs(title = "Average Usage of Words Per Tweet Per State (Data generated at 3:00PM EST 12/10/18)") +
    scale_fill_gradient("Average usage")
```

We also created the following histogram for our `time_data_read` defined earlier. The plot displays the proportion of tweets with a selected word in them for each hour from 6AM to 7PM PST in California on 12/10/18. Again, presenting data on our simple words in this format lends itself to the imagination; one can imagine how people would be tweeting and thinking differently at different times of day and the brain can come up with many stories to explain these differences. For example, notice that tweets with day in themare in a higher proportion during the day in our sampleâ€”a logical connection. But also tweets with student in them seem to happen in higher proportion in early afternoon in our sample, what story can be imagined here?
```{r}
selected_word_times <- '# Enter word here'

ggplot(time_data_read, aes_string(x='Time', y=selected_word_time)) + geom_line() + geom_point() +
       labs(title = "Average Usage of Words Per Tweet Per Hour (Data generated for California from 6:00AM - 7:00PM PST 12/10/18)") +
       scale_x_continuous("Hour (24 hr format)", breaks = c(6,7,8,9,10,11,12,13,14,15,16,17,18,19)) +
       scale_y_continuous("Average Word Usage Per Tweet")

```

Finally, we also made the following image (not generated in R) to display the more artistic side of such a visual analysis. This is an ostensibly simple graphic, with an icon appearing around several global cities, but like all our other results it is more evocative to the human eye/mind than perhaps it should be.
```{r}
map_graphic <- image_read('IconMap.png')
print(map_graphic)
```


V. CONCLUSIONS


We never intended to make any concrete scientific/statiscal conclusions about regions based on the word usages we identify. Such an analysis would require time, resources and expertise beyond the scope of this project in order to be rigorously done. However, we did set out to discover how changing the display of language and information affects the presentation of data. To achieve this goal, our project shows pictures representing commonly recently tweeted words for a given region in addition to some other graphics using static data. All of these products show how, through the process of altering, simplyifing and transforming data one can make it more relatable, interesting, and paradoxically appear more meaningful. While this last fact would be dangerous if we intended to use our project to mislead (i.e. say that because Californians were tweeting about art commonly it would be easy to convince with our graphics the artisitc nature of Californians!), we instead want to show merely the power of this kind of data presentation. In order to truly appreciate the results of our app, one must think about the kind of stories their brain wants to tell about the simple results we produced. Since our question, analysis, and conclusion are so subjective and artistically vague, it is hard to make any singular point, but insofar as we can, we would like to emphasize the impact of PicBook as its ability to engage one in how they think about data. After all, with so much noise around us it is important to remember the power of simplicity and relatability while also realizing the inherent human desire to draw connections.

Using the Picbook application we made, we collected data and created metrical graphics in order to examine our initial question of the relationship between words and images. Our first tab was a map displaying the percentage of tweets in which a word (selected from a drop menu) was used across different states. The second tab shows a graphical chart, which showed how the frequency of a selected wordâ€™s usage varied throughout a selected day. A third tab shows a world map, which features the names and locations of major cities along with their top most used word and icon. (Unlike the interactive Shiny app, which collects live data, the information displayed in the three auxiliary tabs was manually collected in advance and thus shows only a static representation of data for the specific date and time it was collected.) What we found is that, indeed, an individual word appears more meaningful in its representation of its corresponding region when displayed alongside a visual, even if this is not actually the case. 

Due to the simplicity of the app and its functionality, rather than implied conclusions about tweeting patterns, being the primary goal of our project, there are obvious limitations to our app that perhaps undercut the significance of the generated results. Since our app displays results only within range of the 100 nouns we coded into the application, there are likely many nouns that are much more frequently used outside of this range that are not acknowledged by Picbook. In addition, only up to 500 of the most recently published tweets are sampled in each execution, among the countless number of tweets that are published per second. Thus, we can only scratch at the vast sea of noise that is Twitter; any results are not nearly representative of the whole body of tweets. Furthermore, because the pre-coded nouns are all in English and, thus, only results for English-language tweets were generated, the results may not be representative of locations in which English is not a primary written language. Picbook is also unable to acknowledge the presence of typos which may have contributed to a word being identified more or less often, or the variance in meaning of a word across languages and context. Thus, the results generated by Picbook cannot be used to draw any significant conclusions about a location and its tweeting patterns.
